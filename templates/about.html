<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../static/img/favicon.png">

    <title>The sarcasm detector</title>

    <!-- Bootstrap -->
    <link href="../static/css/bootstrap.css" rel="stylesheet">
	<link href="../static/css/bootstrap-theme.css" rel="stylesheet">

    <!-- siimple style -->
    <link href="../static/css/style.css" rel="stylesheet">
    
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index" >Home</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">
            <li><a href="about">About</a></li>
			<li><a href="contact">Contact</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

	<div id="header">
		<div class="container">
			<div class="row">
				<div class="col-lg-6">
					<h1>Co-Attention Mechanism Based On Multi-gram for sarcasm detection</h1>
					<h2 class="subtitle">Learning sarcasm from social media ~</h2>

			</div>
		</div>
	</div>

<br/>

<div class="container4" >
			<form class="form-search" style="text-align:center;" >	

	<div class="contentTitle"> Background</div>   

<br />
          <div class="contentText">	 Sarcasm began in our daily conversations, with the development of social media, Internet users have also used this rhetoric methods to express their opinions and express their feelings. The role of Sarcasm can be summarized : Euphemistically concealed hostility and enhanced influence. For example, this sentence  in [fig] :Really looking forward to that algebra test tomorrow. For most students, the exam is worrying, but he used "looking forward" to express his own uniqueness, if directly Expressing his worrying emotions, the articles sent out are too mediocre and will not attract the attention of others.
<br/>

        Sarcasm detection is a very new task, which was opened in 2014 by the US Secret Service .it was looking for a sarcasm detector to improve their intelligence coming from Twitter. And sarcasm detection has not stopped of this case, once the sentence containing sarcasm will affect the detection accuracy of opinion mining, ordinary emotional analysis task in NLP. Therefore, sarcasm detection has important significance in tasks in opinion mining and emotional analysis. It is classified as a sub-task of sentiment analysis now, but different from like emotional classification task and positive negative classification task. Because the true emotions expressed in satirical sentences are often contrary to the emotions embodied in the surface information.

</div> 


<div class="contentTitle">   It's difficult !!!</div>
<br />
<div class="contentText">	Neuropsychology researchers said:In our daily interactions，People relying on the contrast and incongruity of    shared knowledge and state-of-mind , personality ,facial expressions and prosody to hint the sarcastic intent.

<br/>

Sarcasm detection is a very challenging task in NLP, it's still in its infancy, since we do not have access to such paralinguistic cues, detecting sarcasm in written text is a much harder task.. The most important feature of sarcasm detection  is  context  incongruity, it is a necessary condition for the composition of sarcasm text.
</div> 

<br/>
<div class="contentTitle">   Related work 　</div>
<br />
Machine learning
<div class="contentText">Ghosh et al.  collected the sarcasm data from twitter comments for contextual enviroment task,  the external information contained, the Original text being commented  and the original text author's personality.Therefore, this model can use the situation from original text, sarcasm data and original text author’s personality to modeling .
<br/>
The historical behavior is  prepared for the incongruity. The semantic search tree is used to find the NNP in the sentence as the entity to be described. The external information is the article that the same author has previously described this same entity, determine whether it is sarcasm by comparing the   entity emotion with  current and all previous. However, these methods are often limited and inefficiently modeled
</div>
Deep learning
<div class="contentText">
In recent years, it has been the most popular year of deep learning. As a binary classification task, sarcasm detection also uses such as CNN LSTM attention  neural network methods. Their essence is the high-dimensional features converter, which can  achieve any nonlinear transformation theoretically. People often combine various neural network models to get an encoder that combines the advantages of these neural networks. It depth automatically extracts the style of the sentence and greatly improves the modeling efficiency. Compared with the above method, although the efficiency of modeling is improved, the most important thing is that it isn’t specifically for the ‘incongruity’ to make the decision, Instead, it is a feature that automatically searches from the sentence, So it may ignore the incongruity. Secondly, in my experiment, when the incongruity of the word distance is far away, even if we have LSTM GRU Which improved version of the RNN that can prevents forgetting, it is still difficult to detect the sarcasm text. Despite this, the two-layer cnn rnn model from [ghosh veal] before April 2018 still occupied the state of the art for two years. The most advanced model is the co-attention and co- attention + bilstm [taoxiong]. They compute the embedding similarity between every word-to-word pair in the input sentence to search for conflict sentiments. And the searched conflicts are given back to each word in the form of weights, allowing the model to pay more attention to the incongruity  feature.
</div>

<div class="contentTitle">   My idea motivation　</div>
<br/>
<div class="contentText">
XXX solved a common incongruity pattern. Maybe this method is useful for some of the data, but automatic sarcasm detection is not simple like that. The complexity of automatic detection mainly comes from the difficulty in identifying its appropriate contextual dependence . Simply put, incongruity have different usages in different contexts. In summary, there are three situations in total: (1) an explicit contrast between sentiments or (2) disparity between the conveyed emotion and the author’s situation (context). Prior work has considered sarcasm to be a contrast between a positive and negative sentiment (Riloff et al., 2013).  (3) Between the  two author’s situation  
For example, I abosolutely love to be ignored. The incongruity is ‘love’and ‘ignored’ (sentiment, emotion), study whole night is very happy, the incongruity is ‘ study’ and ‘night’, (situation, emotion).Perfect movie for people who can’t fall asleep , the incongruity is ‘movie’ and ‘asleep’ (situation, situation)
<br/>
<br/>

Ｔay said：Sarcasm relies a lot on the semantic relationships (and contrast) between individual words[] .I don't think this sentence is correct. I think that sometimes the complete above and below , complete  phrases and sub sentence can express the author's situation and emotions.
<br/><br/>
Perfect movie  for people who can’t  fall asleep
<br/>
study whole night is very happy
<br/>
Last minute to buy a gift, #organized
<br/><br/>
Still that example: there is nothing incongruity between movie and asleep . Watching a movie to fall asleep is a very common thing. The real incongruity lies in Perfect movie and people who can’t fall asleep.  This statement more representative of the waste that best movies used to make people fall asleep? Another simple example , study whole night is very happy. Perhaps the word ‘study’ can have a little sad meaning after a strong pre-trained word embedding, but ‘’whole night’ is more in opposition to the word 'happy’. So Tay et al. focuses on word-to-word, and I think we should also focus on phrase to phrase and phrase to word. This can make the machine detect the inconsistency, more complete and more sensitive.
</div>
<br/>
<div class="contentTitle">   My MCCO model　</div>
<br/>
<div class="contentText">Tay et al., Xiong et al. used a co- attention network to find incongruity [][],I think attention is the most heuristic, intuitive, and suitable mechanism for finding and learning sarcasm,We follow the co-attention mechanism from Tay et al. [35].However, the general attention mechanism only pays attention to a single word to a single word, the situation and emotions not replaced by a single word at sometimes,I added a phrase-level feature before the single-word input into the co-attention neural network.We use Cnn learns the encoding of the phrase-level in a sentence and a special max-pooling method to dimensionality reduction, so that the number of parameters generated by adding a large number of phrases is greatly reduced.
We propose the method called M(MAX)C(convolution)CO(co-attention)---MCCO
<br/>
 In order to prepare our special max-pooling, we need to control their time step all same of the convolution process.It is necessary to do a 0-padded first in the sentence beginning of the unigram.
Before the start of the trigram,we also need to add 0-padded at the end and start of the sentence.For the t-th word， the formula is:
<br/>
公式
<br/>
where qtp屬於nxd，the same word changes back to the original size.We can understand this problem in another way. When we train the word embedding ,we use a huge amount of dataset to change the word from one-hot encoder to low-dimensional vector depending on the context, so that the trained words will be a vector that many contexts mean together due to the difference of the training corpus.The phrase-level information we get in this way can be thought of as a single word with embedding that best fits the meaning of the current sentence.
<br/>









<div class="contentTitle"> The seven datasets .</div>
<br />
Reddit:
<br />
<div class="contentText">Reddit is one of the world’s largest online communities with millions of users and hundreds and thousands of topics. Provided by Khodak et al. [ ], posts in Reddit dataset are annotated based on the ‘/s’ tag left by authors themselves. We use two subsets: /r/movies and /r/technology that are tested by Tay et al.and Xiong et al[] [ ] to make our experiment results comparable </div>

IAC(Internet Argument Corpus): 
<br />
<div class="contentText">The original corpus was collected by Walker et al. [] from the online debate forum 4forums.com to study political debates on online communities, Lukin and Walker annotate this corpus for sarcasm detection [24]. We test our model on the same two versions of the dataset used by Tay et al. [34], denoted as IAC-V1 and IAC-V2 respectively .</div>
<br />
Twitter
<br />
<div class="contentText">Twitter is the world’s biggest microblogging platform where users can share any of their news, ideas and thoughts with their followers. Two twitter based datasets are widely used by NLP scholars: Riloff et al. provide a dataset with sentences manually annotated and checked, it is also known as the very first public tweets sarcasm dataset [34]; Ghosh and Veale collect a dataset in which tweets are labeled by hash-tags such as #sarcasm, #sarcastic and #ironie [29], and they also employ a feedback-based system which allows them to contact tweet authors to examine the validity of the sarcasm labels.</div>




<div class="contentText">To train an algorithm to detect sarcasm, we first need some data to train our algorithm on.  Classification is a supervised learning exercise, which means we need to have some sentences labeled as sarcastic and sentences labeled as non-sarcastic so that our classifier can learn the difference between the two.  One option would be to go over an online corpus which might contain some sarcastic sentences, for example online reviews or comments, and label the sentences by hand.  This can be a very tedious exercise if we want to have a large data set.  The other option is to rely on the people writing the sentences to tell us whether their sentences are sarcastic or not.  This is what we are going to do.  The idea here is to use the <a href="https://dev.twitter.com/" target="_top">Twitter API</a> to stream tweets with the label #sarcasm, these will be our sarcastic texts, and other tweets that don't have the label #sarcasm, these will be our non-sarcastic texts.   
<br/><br/>

The obvious advantage of taking our data from Twitter is that we can have as many samples as we want.  Every day people write new sarcastic tweets, we can simply stream them and store them in a database.  I ended up collecting 20 000 clean sarcastic tweets and 100 000 clean non-sarcastic tweets over a period of three weeks in June-July 2014 (see section below to understand what a clean tweet is).  Since tweets are often about what is currently happening in the world, it is important to collect the positive (sarcastic) and negative (non-sarcastic) samples during the same time period in order to isolate the sarcasm variable.  
<br /><br/>

However there is a drawback to taking our data from Twitter; it's noisy.  Some people use the #sarcasm hashtag to point out that their tweet was meant to be sarcastic, but a Human would not have been able to guess that the tweet is sarcastic without the label #sarcasm (example: <i>What a great summer vacation I've been having so far :) #sarcasm</i>).  One may argue however that this is not really noise since the tweet is still sarcastic, at least according to the tweet's owner, and that sarcasm is in the eyes of the beholder.  The converse also happens, someone may write a tweet which is clearly sarcastic but without the label #sarcasm.  There are also instances of sarcastic tweets where the sarcasm is in a linked picture or article.  Sometimes tweets are responses to other tweets, in which case the sarcasm can only be understood within the context of the previous tweets.  Sometimes the label #sarcasm is meant to indicate that, while the tweet itself is not sarcastic, some of its hashtags are (example: <i>Time to do my homework #yay #sarcasm</i>).  I will discuss in the next section how to remove most of that noise, but short of reading all the tweets and labeling them by hand we cannot remove all the noise.   </div>

<div class="contentTitle">  Preprocessing the data</div>
<br />

<div class="contentText">Before extracting features from our text data it is important to clean it up.  To remove the possibility of having sarcastic tweets in which the sarcasm is either in an attached link or in response to another tweet, we simply discard all tweets that have http addresses in them and all tweets that start with the @ symbol. Ideally we would only collect tweets that are written in English.  When we collect sarcastic tweets, the requirement that it contains the label #sarcasm makes it very likely that the tweet will be in English.  To maximize the number of English tweets when we collect non-sarcastic tweets, we require that the location of the tweet is either San-Francisco or New-York.  In addition to these steps, we remove tweets which contain Non-ASCII characters. We then remove all the hashtags, all the friend tags and all mentions of the word sarcasm or sarcastic from the remaining tweets.  If after this pruning stage the tweet is at least 3 words long, we add it to our dataset.  We add this last requirement in order to remove some noise from the sarcastic dataset since I do not believe that one can be sarcastic with only 2 words.  Finally, we remove duplicates.  
</div>

 <div class="contentTitle">  Feature engineering</div>
<br />
<div class="contentText">This is really the meat of the algorithm.  The question here is, what are the variables in a tweet that make it sarcastic or non-sarcastic? And how do we extract them from the tweet?  To this end I engineered several features that might help the classification of tweets and I tested them on a <a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)" target="_top">cross-validation</a> set (I will discuss metrics for evaluating cross-validation in a later section).  The most important features that came out of this analysis are the following:
<br /><br/>
<b>n-grams</b>: More precisely, unigrams and bigrams.  These are just collections of one word (example: <i>really</i>, <i>great</i>, <i>awesome</i>, etc.) and two words (example: <i>really great</i>, <i>super awesome</i>, <i>very weird</i>, etc.).  To extract those, each tweet was <a href="http://en.wikipedia.org/wiki/Tokenization" target="_top">tokenized</a>, <a href="http://en.wikipedia.org/wiki/Stemming" target="_top">stemmed</a>, uncapitalized and then each n-gram was added to a binary feature dictionary. 
<br /><br/>
<b>Sentiments</b>:  My hypothesis here is that sarcastic tweets might be more negative than non-sarcastic tweets or the other way around.  Moreover, there is often a big contrast of sentiments in sarcastic tweets.  What I mean by this is that tweets often start with a very positive sentiment and end with a very negative sentiment (example: <i>I love being cheated on #sarcasm</i>).  <a href="http://en.wikipedia.org/wiki/Sentiment_analysis" target="_top">Sentiment analysis</a> of tweets is a subject on its own so the idea here is to have something simple that can test my hypothesis.  To this end I first split each tweet in one, two and three parts, and then do a sentiment analysis on all parts of the three splittings.  I used two distinct sentiment analyzers.  The first one is my own quick and dirty implementation which uses the <a href="http://sentiwordnet.isti.cnr.it/" target="_top">SentiWordNet</a> dictionary.  This dictionary gives a positive and a negative sentiment score to each word of the English language.  By looking up words in this dictionary, we can give a sentiment score to each part of the tweets.  The other implementation of the sentiment analysis used the python library <a href="http://textblob.readthedocs.org/en/dev/" target="_top">TextBlob</a> which has a built-in sentiment score function. 
<br /><br/>
<b>Topics</b>: There are words that are often grouped together in the same tweets (example: <i>saturday</i>, <i>party</i>, <i>night</i>, <i>friends</i>, etc.).  We call these groups of words topics.  If we first learn the topics, then the classifier will just have to learn which topics are more associated with sarcasm and that will make the supervised learning easier and more accurate.  To learn the topics, I used the python library <a href="http://radimrehurek.com/gensim/" target="_top">gensim</a> which implements topic modeling using <a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" target="_top">latent Dirichlet allocation</a> (LDA).  We first feed all the tweets to the topic modeler which learns the topics.  Then each tweet can be decomposed as a sum of topics, which we use as features.  </div>


<div class="contentTitle">  Choosing a classifier</div>
<br />
<div class="contentText"> There is a very wide range of machine learning algorithms to choose from, most of which are available in the python library <a href="http://scikit-learn.org/stable/" target="_top">Scikit-learn</a>.  However, most of the implementations of these algorithms do not accept sparse matrices as inputs, and since we have a large number of nominal features coming from our n-grams features it is imperative that we encode our features in a sparse matrix.  Out of the algorithms that do support sparse matrices in Scikit-learn, I ended up trying naive Bayes, logistic regression and <a href="http://en.wikipedia.org/wiki/Support_vector_machine" target="_top">support vector machine</a> (SVM) with a linear kernel.  I got the best results in cross validation using SVM with an euclidean regularization coefficient of 0.1.  

<br /><br/>

The metric I used to guide my cross-validation is the <a href="http://en.wikipedia.org/wiki/Precision_and_recall" target="_top">F-score</a>.  This is a good metric when we have a lot more samples from one category than from the other categories.  In our case we have 5 times more non-sarcastic tweets than sarcastic tweets.  If we just use for our metric the accuracy, that is the number of correct predictions divided by the total number of tweets in our cross-validation set, then a simple classifier which always predicts the tweets as non-sarcastic would get a 83% accuracy. This is obviously very misleading so we need a better metric.  We can do better by considering precision and recall for the sarcastic category.  Precision is the number sarcastic tweets correctly identified divided by the total number of tweets classified as sarcastic, while recall is the number sarcastic tweets correctly identified divided by the total number of sarcastic tweets in the cross validation set.  Both precision and recall would be equal to 0% with a dumb classifier which always predicts tweets to be non-sarcastic, so these are already much better scores to quantify the quality of a sarcasm classifier. The F-score is simply the harmonic mean of precision and recall.</div>

<div class="contentTitle">  Results and insights</div>
<br />

<div class="contentText">
To understand the relative importance of each set of features, we can train our algorithm using only n-grams, only the sentiments or only the topics and look at the corresponding F-scores. For the n-grams we get F = 0.56, while for the sentiments we get F = 0.41 and for the topics we get F = 0.35. Note that the addition of all the features will not give an F-score which is a linear addition of the individual F-scores, but we should expect an improvement by combining the features together.  Initially I thought that the main features would come from the sentiment analysis but the n-grams are by far the most important features.  This shows that there is a vocabulary associated with sarcasm after all and it's not just in the tone!

<br /><br/>

To gain some insight into what the algorithm has learned, we can look at the coefficients of our features in the trained SVM to see which ones are the most important.  For n-grams, the features that are the most important to classify sarcastic tweets are <i>just what</i>, <i>yay</i>, <i>perfect time</i>, <i>a blast</i>, <i>shocker</i>, <i>just love</i>, etc.  These n-grams all make perfect sense, but since I streamed the tweets during the 2014 FIFA World Cup I also ended up with three relevant unigrams for sarcastic tweets that are somewhat unexpected: <i>rooney</i>, <i>brazil</i> and <i>suarez</i>.  The most relevant n-grams for the classification of non-sarcastic tweets are <i>feel good</i>, <i>spend some</i>, <i>pray</i>, <i>too funny</i>, <i>be nice</i>, <i>goodnight</i>, etc. For the sentiment analysis, the classifier learned that sarcastic tweets are overall slightly more positive than non-sarcastic tweets and that the first half of a sarcastic tweet is more often positive while the last half of a sarcastic tweet is more often negative.  However what seems to matter the most is the subjectivity which is also measured by the library Textblob.  This means that sarcastic tweets are more about expressing feelings, either positive or negative, than non-sarcastic tweets.  Finally, the topic modeler learned that topics which contain words such as <i>love</i>, <i>life</i>, <i>today</i>, <i>lol</i>, <i>feel</i>, <i>bad</i> etc. are more associated with sarcastic tweets while topics which contain words such as <i>jersey</i>, <i>procrastinating</i>, <i>hot</i>, <i>complain</i>, <i>storm</i>, <i>mom</i>, etc. are more associated with non-sarcastic tweets.  

<br /><br/>

After the cross-validation phase, I tested the sarcasm classifier on a separate test set made of 24 000 tweets and obtained an <b>F-score of 0.60</b> on the sarcastic category.  This is a small improvement over the previous studies in sarcasm detection using tweets which obtained F-scores in the range 0.50-0.55, see references <a href="#reference">[1,2,3,4]</a>.  The previous work on sarcasm detection uses only n-grams features and/or rule-based classification algorithms.  Our analysis improves over those previous ones by incorporating a sentiment decomposition analysis and a topic modeling analysis.  Moreover, we used a modern machine learning algorithm, SVM, which is not rule-based.
</div>

<div class="contentTitle">  Outlook
</div>
<br />
<div class="contentText"> I think sarcasm detection is a really fascinating subject, and I'm not being sarcastic!  It's not clear to me whether or not this sarcasm detector would be useful for the U.S. secret service, but I do feel confident that I answered my original question; it is possible to detect sarcasm using tools from NLP.  One quick way to improve this detector would be to incorporate a spell check for the tweets.  This should reduce the number of dimensions of the dictionary for the n-gram features and improve the sentiment analysis as well.  I did some preliminary experiments with the spell check of the library Textblob but I did not get any improvements using it, perhaps a better spell checker would do better.  
</div>

<div class="contentTitle">  The tools used</div>
<br />
<div class="contentText">Here is a laundry list of tools used in this project:<br /><br/>
<b>Back end</b>: Python with Numpy, Scipy, Scikit-learn, NLTK, gensim, Textblob and tweepy. <br /><br/>
 <b>Front end</b>: Python with Flask, HTML, CSS, Javascript and the whole thing hosted by <a href="https://www.pythonanywhere.com/" target="_top">pythonanywhere.com</a>.  
<br /><br/>

The code can be found on <a href="https://github.com/MathieuCliche/Sarcasm_detector" target="_top">https://github.com/MathieuCliche/Sarcasm_detector</a>
</div>

<div class="contentTitle">  References <a name="reference"></a>
</div>
<div class="contentText">
<br />
[1] Dmitry Davidov, Oren Tsur and Ari Rappoport, <a href="http://dl.acm.org/citation.cfm?id=1870568.1870582
" target="_top"><i>Semi-Supervised Recognition of Sarcastic Sentences
in Twitter and Amazon</i></a>
<!-- pattern matching, F=0.545 --><br/>
[2] Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert and Ruihong Huang, <a href="http://www.cs.utah.edu/~huangrh/official-sarcasm-cameraReady-v2.pdf" target="_top"><i>Sarcasm as Contrast between a Positive Sentiment and Negative Situation</i></a>
<!-- positive sentiment negative situation, F=0.51--><br/>
[3] Roberto Gonzalez-Ibanez, Smaranda Muresan and Nina Wacholder,<a href="http://dl.acm.org/citation.cfm?id=2002850
" target="_top"><i> Identifying Sarcasm in Twitter: A Closer Look</i></a>
<!-- unigrams and emoticons 70% accuracy --><br/>
[4] Christine Liebrecht, Florian Kunneman  and Antal Van den Bosch, <a href="http://www.aclweb.org/anthology/W13-1605" target="_top"><i>The perfect solution for detecting sarcasm in tweets #not</i></a>
<!-- Deutch paper--> <br/>
</div>
<br/>
<br/>

		</form>
	</div>
</div>

    <script src="../static/js/jquery-1.11.1.min.js"></script>
    <script src="../static/js/bootstrap.min.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53465548-1', 'auto');
  ga('send', 'pageview');

</script>

  </body>
</html>
